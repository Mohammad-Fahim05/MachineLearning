{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV7PD3-3lltx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    log_loss,\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "RANDOM_STATE = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/train.csv\")\n",
        "test  = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/test.csv\")\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Test shape:\", test.shape)\n",
        "\n",
        "train.head()\n"
      ],
      "metadata": {
        "id": "gskJEbjmluET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMissing Values:\\n\", train.isnull().sum())\n",
        "print(\"\\nDuplicate rows:\", train.duplicated().sum())\n",
        "\n",
        "train = train.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "a0QZrN_klw3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_COL = \"Status\" # change here\n",
        "ID_COL = \"id\"\n",
        "\n",
        "print(train[TARGET_COL].value_counts())\n",
        "print(\"Number of classes:\", train[TARGET_COL].nunique())\n"
      ],
      "metadata": {
        "id": "7mYkZJFxmLor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = [\n",
        "    'Drug', 'Ascites', 'Hepatomegaly', 'Spiders',\n",
        "    'Cholesterol', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides'\n",
        "]\n",
        "\n",
        "train.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n",
        "test.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n"
      ],
      "metadata": {
        "id": "xcdNneo1mPJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = train[ID_COL]\n",
        "test_ids  = test[ID_COL]\n",
        "\n",
        "train.drop(columns=[ID_COL], inplace=True)\n",
        "test.drop(columns=[ID_COL], inplace=True)\n",
        "\n",
        "X = train.drop(columns=[TARGET_COL])\n",
        "y = train[TARGET_COL]\n"
      ],
      "metadata": {
        "id": "Y-gynvEmmimg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "\n",
        "print(\"Numeric Features:\", numeric_features)\n",
        "print(\"Categorical Features:\", categorical_features)\n"
      ],
      "metadata": {
        "id": "zkk9kFlDmwGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=y)\n",
        "plt.title(\"Target Class Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w13wDoMMm2O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numeric_features:\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
        "    sns.histplot(train[col], kde=True, ax=ax[0])\n",
        "    sns.boxplot(x=train[col], ax=ax[1])\n",
        "    plt.suptitle(col)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "N8d5z_nCm1_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_features:\n",
        "    sns.countplot(y=train[col])\n",
        "    plt.title(col)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "oK_9pCOPm-WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(train[numeric_features].corr(), cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tV7_fC5tnCs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n"
      ],
      "metadata": {
        "id": "HwrYuG4snEyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_pipeline, numeric_features),\n",
        "    (\"cat\", categorical_pipeline, categorical_features)\n",
        "])\n"
      ],
      "metadata": {
        "id": "SJAl_qF0nG8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc  = le.transform(y_test)\n"
      ],
      "metadata": {
        "id": "H83CmC4tnJbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GradientBoostingClassifier(\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.02,\n",
        "    max_depth=4,\n",
        "    min_samples_split=42,\n",
        "    min_samples_leaf=18,\n",
        "    max_features=0.7,\n",
        "    subsample=0.7,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n"
      ],
      "metadata": {
        "id": "DDFJE261nLT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=14,\n",
        "    min_samples_split=30,\n",
        "    min_samples_leaf=15,\n",
        "    max_features=\"sqrt\",\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n"
      ],
      "metadata": {
        "id": "gPyIDRKkoo36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "model = HistGradientBoostingClassifier(\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    max_iter=500,\n",
        "    min_samples_leaf=25,\n",
        "    l2_regularization=0.2,\n",
        "    max_bins=255,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n"
      ],
      "metadata": {
        "id": "-usiVJm2oooK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------------------#\n",
        "#              XGBOOST MODEL\n",
        "# ---------------------------------------------#\n",
        "model = XGBClassifier(\n",
        "    n_estimators=700,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=6,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    gamma=1,\n",
        "    reg_alpha=0.2,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=42,\n",
        "    objective=\"multi:softprob\",\n",
        "    eval_metric=\"mlogloss\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "OWtrBIuSooe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "model = LGBMClassifier(\n",
        "    n_estimators=1200,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=-1,\n",
        "    num_leaves=50,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_alpha=0.2,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "Eh2Ra69xrP0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    depth=6,\n",
        "    learning_rate=0.03,\n",
        "    loss_function=\"MultiClass\",\n",
        "    eval_metric=\"Accuracy\",\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "VtqvAP0CrUhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"model\", model)\n",
        "])\n"
      ],
      "metadata": {
        "id": "4T00k04-nNkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "cv_scores = cross_val_score(\n",
        "    pipeline,\n",
        "    X_train,\n",
        "    y_train_enc,\n",
        "    scoring=\"neg_log_loss\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"CV Log Loss:\", -cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "BHcY64jOnPS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X_train, y_train_enc)\n"
      ],
      "metadata": {
        "id": "qMAVHKJ7nRTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pipeline.predict(X_test)\n",
        "y_proba = pipeline.predict_proba(X_test)\n",
        "\n",
        "print(\"Log Loss:\", log_loss(y_test_enc, y_proba))\n",
        "print(\"Accuracy:\", accuracy_score(y_test_enc, y_pred))\n",
        "print(\"ROC-AUC (OVR):\", roc_auc_score(y_test_enc, y_proba, multi_class=\"ovr\"))\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test_enc, y_pred, target_names=le.classes_))\n"
      ],
      "metadata": {
        "id": "ma40o868nUX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test_enc, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CbHXgUrYnWNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_proba = pipeline.predict_proba(test)\n",
        "\n",
        "submission = pd.DataFrame(\n",
        "    test_proba,\n",
        "    columns=[f\"Status_{cls}\" for cls in le.classes_]\n",
        ")\n",
        "\n",
        "submission.insert(0, \"id\", test_ids)\n",
        "submission.to_csv(\"submission_pipeline_final.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Submission file created successfully\")\n",
        "submission.head()\n"
      ],
      "metadata": {
        "id": "Qboi868UnYke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(\n",
        "    n_splits=5,\n",
        "    shuffle=True,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "cv_scores = cross_val_score(\n",
        "    pipeline,\n",
        "    X_train,\n",
        "    y_train_enc,\n",
        "    scoring=\"neg_log_loss\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"CV Log Loss:\", -cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "LMoSPxTgoQgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    log_loss,\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# ============================\n",
        "# 1. Load Data\n",
        "# ============================\n",
        "train = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/train.csv\")\n",
        "test = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/test.csv\")\n",
        "\n",
        "# ============================\n",
        "# 2. Drop unwanted columns\n",
        "# ============================\n",
        "cols_to_drop = ['Drug', 'Ascites', 'Hepatomegaly', 'Spiders',\n",
        "                'Cholesterol', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides']\n",
        "\n",
        "train.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
        "test.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# ============================\n",
        "# 3. Fill specific numeric columns\n",
        "# ============================\n",
        "for col in ['Platelets', 'Prothrombin']:\n",
        "    train[col] = train[col].fillna(train[col].mean())\n",
        "    test[col] = test[col].fillna(test[col].mean())\n",
        "\n",
        "# ============================\n",
        "# 4. Separate ID + Target\n",
        "# ============================\n",
        "train_ids = train['id']\n",
        "test_ids = test['id']\n",
        "\n",
        "train.drop(columns=['id'], inplace=True)\n",
        "test.drop(columns=['id'], inplace=True)\n",
        "\n",
        "X = train.drop(columns=['Status'])\n",
        "y = train['Status']\n",
        "\n",
        "# ============================\n",
        "# 5. Train-Test split\n",
        "# ============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 6. Identify numeric + categorical features\n",
        "# ============================\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# ============================\n",
        "# 7. Impute + Scale numeric features\n",
        "# ============================\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_num = scaler.fit_transform(num_imputer.fit_transform(X_train[numeric_features]))\n",
        "X_test_num = scaler.transform(num_imputer.transform(X_test[numeric_features]))\n",
        "test_num = scaler.transform(num_imputer.transform(test[numeric_features]))\n",
        "\n",
        "# ============================\n",
        "# 8. Impute + Encode categorical features\n",
        "# ============================\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "ohe = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "X_train_cat = ohe.fit_transform(cat_imputer.fit_transform(X_train[categorical_features]))\n",
        "X_test_cat = ohe.transform(cat_imputer.transform(X_test[categorical_features]))\n",
        "test_cat = ohe.transform(cat_imputer.transform(test[categorical_features]))\n",
        "\n",
        "# ============================\n",
        "# 9. Combine numeric + categorical\n",
        "# ============================\n",
        "X_train_final = hstack([X_train_num, X_train_cat])\n",
        "X_test_final = hstack([X_test_num, X_test_cat])\n",
        "test_final = hstack([test_num, test_cat])\n",
        "\n",
        "# ============================\n",
        "# 10. Label Encode Target\n",
        "# ============================\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "\n",
        "# ============================\n",
        "# 11. Model Training\n",
        "# ============================\n",
        "model = GradientBoostingClassifier(\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.02,\n",
        "    max_depth=4,\n",
        "    min_samples_split=42,\n",
        "    min_samples_leaf=18,\n",
        "    max_features=0.7,\n",
        "    subsample=0.7,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train_final, y_train_enc)\n",
        "\n",
        "# ============================\n",
        "# 12. Predictions\n",
        "# ============================\n",
        "y_pred = model.predict(X_test_final)\n",
        "y_pred_proba = model.predict_proba(X_test_final)\n",
        "\n",
        "# ============================\n",
        "# 13. Evaluation Metrics\n",
        "# ============================\n",
        "\n",
        "# Log Loss\n",
        "loss = log_loss(y_test_enc, y_pred_proba)\n",
        "print(f\"\\nüîç Log Loss: {loss:.5f}\")\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test_enc, y_pred)\n",
        "print(f\"üéØ Accuracy: {acc:.5f}\")\n",
        "\n",
        "# ROC-AUC (multiclass)\n",
        "roc_auc = roc_auc_score(y_test_enc, y_pred_proba, multi_class='ovr')\n",
        "print(f\"üß≤ ROC-AUC (OVR): {roc_auc:.5f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nüìÑ Classification Report:\")\n",
        "print(classification_report(y_test_enc, y_pred, target_names=le.classes_))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nüß© Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_enc, y_pred))\n",
        "\n",
        "# ============================\n",
        "# 14. Predict on test.csv\n",
        "# ============================\n",
        "probs = model.predict_proba(test_final)\n",
        "class_names = le.classes_\n",
        "\n",
        "submission = pd.DataFrame(probs, columns=[f\"Status_{cls}\" for cls in class_names])\n",
        "submission.insert(0, 'id', test_ids)\n",
        "submission.to_csv(\"submission_no_pipeline_metrics.csv\", index=False)\n",
        "print(\"\\n‚úÖ Submission file created successfully!\")\n",
        "print(submission.head())\n",
        "üîç Log Loss: 0.38177\n",
        "üéØ Accuracy: 0.85033\n",
        "üß≤ ROC-AUC (OVR): 0.91439\n",
        "\n",
        "üìÑ Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           C       0.86      0.94      0.90      2004\n",
        "          CL       0.62      0.12      0.20        67\n",
        "           D       0.82      0.72      0.76       929\n",
        "\n",
        "    accuracy                           0.85      3000\n",
        "   macro avg       0.77      0.59      0.62      3000\n",
        "weighted avg       0.84      0.85      0.84      3000\n",
        "\n",
        "\n",
        "üß© Confusion Matrix:\n",
        "[[1877    1  126]\n",
        " [  34    8   25]\n",
        " [ 259    4  666]]\n",
        "\n",
        "‚úÖ Submission file created successfully!\n",
        "      id  Status_C  Status_CL  Status_D\n",
        "0  15000  0.949418   0.004539  0.046043\n",
        "1  15001  0.980001   0.003756  0.016243\n",
        "2  15002  0.927062   0.008343  0.064594\n",
        "3  15003  0.061800   0.164013  0.774188\n",
        "4  15004  0.980146   0.003517  0.016338"
      ],
      "metadata": {
        "id": "3Z70Hzy1ol2T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}